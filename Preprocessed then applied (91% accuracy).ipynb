{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e52972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    # Create output directories if they don't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Train')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Train'))\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Train', 'pos')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Train', 'pos'))\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Train', 'neg')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Train', 'neg'))\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Test')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Test'))\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Test', 'pos')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Test', 'pos'))\n",
    "    if not os.path.exists(os.path.join(output_dir, 'Test', 'neg')):\n",
    "        os.makedirs(os.path.join(output_dir, 'Test', 'neg'))\n",
    "    \n",
    "    # Process training positive samples\n",
    "    for filename in os.listdir(os.path.join(input_dir, 'Train', 'pos')):\n",
    "        img_path = os.path.join(input_dir, 'Train', 'pos', filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        # Resize image to a fixed size (e.g., 128x64)\n",
    "        img = cv2.resize(img, (128, 64))\n",
    "        # Convert image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply median filter for noise reduction\n",
    "        img = cv2.medianBlur(img, 3)\n",
    "        # Data augmentation: horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "        # Save preprocessed image\n",
    "        cv2.imwrite(os.path.join(output_dir, 'Train', 'pos', filename), img)\n",
    "    \n",
    "    # Process training negative samples\n",
    "    for filename in os.listdir(os.path.join(input_dir, 'Train', 'neg')):\n",
    "        img_path = os.path.join(input_dir, 'Train', 'neg', filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        # Resize image to a fixed size (e.g., 128x64)\n",
    "        img = cv2.resize(img, (128, 64))\n",
    "        # Convert image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply median filter for noise reduction\n",
    "        img = cv2.medianBlur(img, 3)\n",
    "        # Data augmentation: horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "        # Save preprocessed image\n",
    "        cv2.imwrite(os.path.join(output_dir, 'Train', 'neg', filename), img)\n",
    "\n",
    "    # Process testing positive samples\n",
    "    for filename in os.listdir(os.path.join(input_dir, 'Test', 'pos')):\n",
    "        img_path = os.path.join(input_dir, 'Test', 'pos', filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        # Resize image to a fixed size (e.g., 128x64)\n",
    "        img = cv2.resize(img, (128, 64))\n",
    "        # Convert image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply median filter for noise reduction\n",
    "        img = cv2.medianBlur(img, 3)\n",
    "        # Save preprocessed image\n",
    "        cv2.imwrite(os.path.join(output_dir, 'Test', 'pos', filename), img)\n",
    "    \n",
    "    # Process testing negative samples\n",
    "    for filename in os.listdir(os.path.join(input_dir, 'Test', 'neg')):\n",
    "        img_path = os.path.join(input_dir, 'Test', 'neg', filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        # Resize image to a fixed size (e.g., 128x64)\n",
    "        img = cv2.resize(img, (128, 64))\n",
    "        # Convert image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply median filter for noise reduction\n",
    "        img = cv2.medianBlur(img, 3)\n",
    "        # Save preprocessed image\n",
    "        cv2.imwrite(os.path.join(output_dir, 'Test', 'neg', filename), img)\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = 'INRIAPerson'\n",
    "output_dir = 'Preprocessed_Data'\n",
    "\n",
    "# Perform preprocessing\n",
    "preprocess_images(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d002eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1832\n",
      "Number of HOG features extracted from the first image: 3780\n",
      "Number of HOG features: 3780\n",
      "Number of LBP features: 26\n",
      "Number of LTP features: 16384\n",
      "Accuracy: 85.83%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    pos_folder = os.path.join(folder, 'Train', 'pos')\n",
    "    neg_folder = os.path.join(folder, 'Train', 'neg')\n",
    "    \n",
    "    for filename in os.listdir(pos_folder):\n",
    "        img = cv2.imread(os.path.join(pos_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Positive class\n",
    "\n",
    "    for filename in os.listdir(neg_folder):\n",
    "        img = cv2.imread(os.path.join(neg_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Negative class\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets from Preprocessed_Data folder\n",
    "images, labels = load_images_from_folder('Preprocessed_Data')\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        f = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False, feature_vector=True)\n",
    "        hog_features.append(f)\n",
    "    \n",
    "    print(f\"Number of images: {len(images)}\")\n",
    "    print(f\"Number of HOG features extracted from the first image: {len(hog_features[0])}\")\n",
    "    \n",
    "    return np.array(hog_features)\n",
    "\n",
    "# Function to extract LBP features\n",
    "def extract_lbp_features(images):\n",
    "    lbp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        lbp = local_binary_pattern(gray, 24, 8, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 24 + 3), range=(0, 24 + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-6)\n",
    "        lbp_features.append(hist)\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Function to extract LTP features\n",
    "def extract_ltp_features(images):\n",
    "    ltp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ltp_up = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) >= 0.05, 1, 0)\n",
    "        ltp_down = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) <= -0.05, 1, 0)\n",
    "        ltp_feat = np.hstack((ltp_up.ravel(), ltp_down.ravel()))\n",
    "        ltp_features.append(ltp_feat)\n",
    "    return np.array(ltp_features)\n",
    "\n",
    "# Combine HOG and LBP features\n",
    "def combine_features(images):\n",
    "    hog_features = extract_hog_features(images)\n",
    "    lbp_features = extract_lbp_features(images)\n",
    "    ltp_features = extract_ltp_features(images)\n",
    "    \n",
    "    print(f\"Number of HOG features: {hog_features.shape[1]}\")\n",
    "    print(f\"Number of LBP features: {lbp_features.shape[1]}\")\n",
    "    print(f\"Number of LTP features: {ltp_features.shape[1]}\")\n",
    "    \n",
    "    combined_features = np.hstack((hog_features, lbp_features, ltp_features))\n",
    "    return combined_features\n",
    "\n",
    "# Extract combined features\n",
    "features = combine_features(images)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_normalized, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create and train the SVM with an RBF kernel\n",
    "classifier = svm.SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8ab374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1832\n",
      "Number of HOG features extracted from the first image: 3780\n",
      "Number of HOG features: 3780\n",
      "Number of LBP features: 26\n",
      "Number of LTP features: 16384\n",
      "Accuracy: 92.91%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    pos_folder = os.path.join(folder, 'Train', 'pos')\n",
    "    neg_folder = os.path.join(folder, 'Train', 'neg')\n",
    "    \n",
    "    for filename in os.listdir(pos_folder):\n",
    "        img = cv2.imread(os.path.join(pos_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Positive class\n",
    "\n",
    "    for filename in os.listdir(neg_folder):\n",
    "        img = cv2.imread(os.path.join(neg_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Negative class\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load datasets from Preprocessed_Data folder\n",
    "images, labels = load_images_from_folder('Preprocessed_Data')\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        f = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False, feature_vector=True)\n",
    "        hog_features.append(f)\n",
    "    \n",
    "    print(f\"Number of images: {len(images)}\")\n",
    "    print(f\"Number of HOG features extracted from the first image: {len(hog_features[0])}\")\n",
    "    \n",
    "    return np.array(hog_features)\n",
    "\n",
    "# Function to extract LBP features\n",
    "def extract_lbp_features(images):\n",
    "    lbp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        lbp = local_binary_pattern(gray, 24, 8, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 24 + 3), range=(0, 24 + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-6)\n",
    "        lbp_features.append(hist)\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Function to extract LTP features\n",
    "def extract_ltp_features(images):\n",
    "    ltp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ltp_up = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) >= 0.05, 1, 0)\n",
    "        ltp_down = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) <= -0.05, 1, 0)\n",
    "        ltp_feat = np.hstack((ltp_up.ravel(), ltp_down.ravel()))\n",
    "        ltp_features.append(ltp_feat)\n",
    "    return np.array(ltp_features)\n",
    "\n",
    "# Combine HOG and LBP features\n",
    "def combine_features(images):\n",
    "    hog_features = extract_hog_features(images)\n",
    "    lbp_features = extract_lbp_features(images)\n",
    "    ltp_features = extract_ltp_features(images)\n",
    "    \n",
    "    print(f\"Number of HOG features: {hog_features.shape[1]}\")\n",
    "    print(f\"Number of LBP features: {lbp_features.shape[1]}\")\n",
    "    print(f\"Number of LTP features: {ltp_features.shape[1]}\")\n",
    "    \n",
    "    combined_features = np.hstack((hog_features, lbp_features, ltp_features))\n",
    "    return combined_features\n",
    "\n",
    "# Extract combined features\n",
    "features = combine_features(images)\n",
    "\n",
    "# Data augmentation (manually)\n",
    "augmented_features = []\n",
    "augmented_labels = []\n",
    "\n",
    "for feature, label in zip(features, labels):\n",
    "    augmented_features.append(feature)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    # Augment feature by adding random noise (example augmentation)\n",
    "    augmented_feature = feature + np.random.normal(loc=0, scale=0.01, size=feature.shape)\n",
    "    augmented_features.append(augmented_feature)\n",
    "    augmented_labels.append(label)\n",
    "\n",
    "# Convert augmented data to numpy arrays\n",
    "augmented_features = np.array(augmented_features)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(augmented_features)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_normalized, augmented_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create and train the SVM with an RBF kernel\n",
    "classifier = svm.SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f7b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.91%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       466\n",
      "           1       0.95      0.85      0.90       267\n",
      "\n",
      "    accuracy                           0.93       733\n",
      "   macro avg       0.93      0.91      0.92       733\n",
      "weighted avg       0.93      0.93      0.93       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ca35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected target: 241\n",
      "False detections: 13\n",
      "Missed detections: 39\n",
      "Semi-accurate number: 681\n",
      "Accurate number: 228\n",
      "False detection rate: 0.03\n",
      "Missed detection rate: 0.15\n",
      "Semi-accuracy rate: 0.93\n",
      "Total: 733\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mentioned metrics\n",
    "detected_target = sum(y_pred)  # Number of positive predictions\n",
    "false_detections = sum((y_pred == 1) & (y_test == 0))  # False positive predictions\n",
    "missed_detections = sum((y_pred == 0) & (y_test == 1))  # False negative predictions\n",
    "semi_accurate_number = sum(y_pred == y_test)  # Number of semi-accurate predictions\n",
    "accurate_number = sum((y_pred == 1) & (y_test == 1))  # True positive predictions\n",
    "\n",
    "# Calculate rates\n",
    "false_detection_rate = false_detections / sum(y_test == 0)\n",
    "missed_detection_rate = missed_detections / sum(y_test == 1)\n",
    "semi_accuracy_rate = semi_accurate_number / len(y_pred)\n",
    "\n",
    "\n",
    "# Calculate total\n",
    "total = len(y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Detected target: {detected_target}\")\n",
    "print(f\"False detections: {false_detections}\")\n",
    "print(f\"Missed detections: {missed_detections}\")\n",
    "print(f\"Semi-accurate number: {semi_accurate_number}\")\n",
    "print(f\"Accurate number: {accurate_number}\")\n",
    "print(f\"False detection rate: {false_detection_rate:.2f}\")\n",
    "print(f\"Missed detection rate: {missed_detection_rate:.2f}\")\n",
    "print(f\"Semi-accuracy rate: {semi_accuracy_rate:.2f}\")\n",
    "print(f\"Total: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49528afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[453  13]\n",
      " [ 39 228]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test contains the true labels and y_pred contains the predicted labels\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31daef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss rate: 14.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP)\n",
    "TP = cm[1, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "# Calculate miss rate (false negative rate)\n",
    "miss_rate = (FN / (TP + FN)) * 100\n",
    "\n",
    "print(f\"Miss rate: {miss_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d2dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed detection rate: 14.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP)\n",
    "TP = cm[1, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "# Calculate missed detection rate (false negative rate)\n",
    "missed_detection_rate = (FN / (TP + FN)) * 100\n",
    "\n",
    "print(f\"Missed detection rate: {missed_detection_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0db34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total detections: 758\n"
     ]
    }
   ],
   "source": [
    "# Calculate Total number of detections\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP)\n",
    "TP = cm[1, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "#total_detections = false_detection_number + missed_detection_number + semi_accurate_number\n",
    "\n",
    "total_detections = FN + 45 + 674\n",
    "\n",
    "print(\"Total detections:\", total_detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    pos_folder = os.path.join(folder, 'Train', 'pos')\n",
    "    neg_folder = os.path.join(folder, 'Train', 'neg')\n",
    "    \n",
    "    for filename in os.listdir(pos_folder):\n",
    "        img = cv2.imread(os.path.join(pos_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Positive class\n",
    "\n",
    "    for filename in os.listdir(neg_folder):\n",
    "        img = cv2.imread(os.path.join(neg_folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Negative class\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load datasets from Preprocessed_Data folder\n",
    "images, labels = load_images_from_folder('Preprocessed_Data')\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        f = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False, feature_vector=True)\n",
    "        hog_features.append(f)\n",
    "    \n",
    "    return np.array(hog_features)\n",
    "\n",
    "# Function to extract LBP features\n",
    "def extract_lbp_features(images):\n",
    "    lbp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        lbp = local_binary_pattern(gray, 24, 8, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 24 + 3), range=(0, 24 + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-6)\n",
    "        lbp_features.append(hist)\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Function to extract LTP features\n",
    "def extract_ltp_features(images):\n",
    "    ltp_features = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (64, 128))  # Resize to standard dimensions\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ltp_up = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) >= 0.05, 1, 0)\n",
    "        ltp_down = np.where((gray - cv2.GaussianBlur(gray, (5, 5), 0)) <= -0.05, 1, 0)\n",
    "        ltp_feat = np.hstack((ltp_up.ravel(), ltp_down.ravel()))\n",
    "        ltp_features.append(ltp_feat)\n",
    "    return np.array(ltp_features)\n",
    "\n",
    "# Combine HOG, LBP, and LTP features\n",
    "def combine_features(images):\n",
    "    hog_features = extract_hog_features(images)\n",
    "    lbp_features = extract_lbp_features(images)\n",
    "    ltp_features = extract_ltp_features(images)\n",
    "    \n",
    "    combined_features = np.hstack((hog_features, lbp_features, ltp_features))\n",
    "    return combined_features\n",
    "\n",
    "# Extract combined features\n",
    "features = combine_features(images)\n",
    "\n",
    "# Data augmentation\n",
    "augmented_features = []\n",
    "augmented_labels = []\n",
    "\n",
    "for feature, label in zip(features, labels):\n",
    "    augmented_features.append(feature)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    # Augment feature by adding random noise (example augmentation)\n",
    "    augmented_feature = feature + np.random.normal(loc=0, scale=0.01, size=feature.shape)\n",
    "    augmented_features.append(augmented_feature)\n",
    "    augmented_labels.append(label)\n",
    "\n",
    "# Convert augmented data to numpy arrays\n",
    "augmented_features = np.array(augmented_features)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(augmented_features)\n",
    "\n",
    "# Fit GMM with fewer components\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)\n",
    "gmm.fit(features_normalized)\n",
    "\n",
    "# Extract GMM parameters as features\n",
    "gmm_features = gmm.predict_proba(features_normalized)\n",
    "\n",
    "# Combine GMM features with existing features\n",
    "combined_features_with_gmm = np.hstack((features_normalized, gmm_features))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features_with_gmm, augmented_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create and train the SVM with an RBF kernel\n",
    "classifier = svm.SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a118b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edccfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
